---
author: "Default Post Author"
layout: "post"
categories: "RL"
title: "Reinforcement Learning as Probabilistic Modelling: A Variational Inference Formulation (Part I) WIP"
permalink: "/:categories/:title/"
usemathjax: true
---

Reinforcement Learning is concerned with an agent attempting to acquire optimal behaviour in unknown environments that typically exhibit stochasticity. Though minimally supervised, reinforcement learning algorithms have shown numerous success ranging from solving ATARI games using Deep Q-Networks, to the triumphant victory against the world champions in the game of GO, and recently in StartCraft.


Maybe the most intuitive formulation of reinforcement learning is that stemming from Andrew and Rich's book that introduces a reinforcement learning agent as that acting in a Markov Decision Process with the aim of learning an optimal action-selection rule or policy. Though interesting, with time I came to appreciate another view on reinforcement learning that has roots in probabilistic modelling and variational inference. I found that such a definition unifies different views on reinforcement learning under one common framework and gives a more formal treatment of the field altogether. I will detail such a formulation and explain some special cases arising, e.g., Max-Entropy Reinforcement Learning, and others. To fully understand such a formulation, the reader needs to have access to background on probabilistic modelling, latent variable models, and the evidence lower bound (ELBO). As such, I split this post into two parts. In this part, we will discuss basics paving the way to the reinforcement learning formulation in Part II.

$$E=mc^2$$

## [{{ page.title }}](Reinforcement Learning as Probabilistic Modelling: A Variational Inference Formulation (Part I) WIP)

